<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <!-- Basic SEO -->
  <meta name="description" content="An efficient framework for attributing global properties of diffusion models to data contributors using sparsified fine-tuning and Shapley value estimation.">
  <meta name="keywords" content="Diffusion models, data attribution, Shapley value, generative models, pruning, fine-tuning, model explanation, ICLR 2025">
  
  <!-- Open Graph for Social Sharing -->
  <meta property="og:title" content="Crediting Data Contributors of Diffusion Models" />
  <meta property="og:description" content="We propose a scalable Shapley-based framework for estimating the contribution of training data to generative model behavior using sparsified fine-tuning." />
  <meta property="og:url" content="https://your-username.github.io/contributor-attribution/" />
  <meta property="og:image" content="static/images/banner.jpg" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  
  <!-- Twitter Card -->
  <meta name="twitter:title" content="Crediting Data Contributors of Diffusion Models" />
  <meta name="twitter:description" content="ICLR 2025 paper proposing a scalable Shapley-based method for estimating data value in diffusion models via pruning and fine-tuning." />
  <meta name="twitter:image" content="static/images/banner.jpg">
  <meta name="twitter:card" content="summary_large_image">
  
  <!-- Responsive layout -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>An Efficient Framework for Crediting Data Contributors of Diffusion Models</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">An Efficient Framework for Crediting Data Contributors of Diffusion Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Chris Lin</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://mingyu-lu.github.io/" target="_blank">Mingyu Lu</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://chanwoo.kim/" target="_blank">Chanwoo Kim,</a>
                    <span class="author-block">
                    <a href="https://aims.cs.washington.edu/su-in-lee" target="_blank">Su-In Lee</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Paul G. Allen School of Computer Science & Engineering<br>University of Washington<br>ICLR 2025</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://openreview.net/pdf?id=9EqQC2ct4H" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/q8888620002/Group-Attribution-for-Diffusion-Models" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2407.03153" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p> As diffusion models are deployed in real-world settings, and their performance is driven by training data, appraising the contribution of data contributors is crucial to creating incentives for sharing quality data and to implementing policies for data compensation. 
            Depending on the use case, model performance corresponds to various global properties of the distribution learned by a diffusion model (e.g., overall aesthetic quality). 
            Hence, here we address the problem of attributing global properties of diffusion models to data contributors. The Shapley value provides a principled approach to valuation by uniquely satisfying game-theoretic axioms of fairness. 
            However, estimating Shapley values for diffusion models is computationally impractical because it requires retraining on many training data subsets corresponding to different contributors and rerunning inference. We introduce a method to efficiently retrain and rerun inference for Shapley value estimation, by leveraging model pruning and fine-tuning. 
            We evaluate the utility of our method with three use cases: (i) image quality for a DDPM trained on a CIFAR dataset, (ii) demographic diversity for an LDM trained on CelebA-HQ, and (iii) aesthetic quality for a Stable Diffusion model LoRA-finetuned on Post-Impressionist artworks. 
            Our results empirically demonstrate that our framework can identify important data contributors across models' global properties, outperforming existing attribution methods for diffusion models.   
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

    <div class="item">
      <div class="carousel-item-wrapper center-text wrapper-70">
        <img src="static/images/concept_fig.jpg" alt="Schematic overview of our method" style="width: 100%; height: auto;" />
        <p class="is-size-6" style="margin-top: 1rem;">
          <strong>Schematic overview of our proposed framework.</strong><br/>
          Let \(\theta^*\) denote a trained diffusion model for which we aim to credit data contributors, and \(\tilde{\theta}^*\) represent a pruned model that approximates \(\theta^*\). After fine-tuning the pruned model on data subsets (yielding \(\tilde{\theta}^{\text{ft}}\)), we re-run inference and evaluate global properties \(\mathcal{F}\) to estimate Shapley values.
        </p>
      </div>
    </div>
    
    <!-- Top Contributors Figure -->
    <div class="item">
        <div class="carousel-item-wrapper center-text wrapper-60">
        <img src="static/images/top_contributors.jpg" alt="Top contributors and generated images" style="width: 100%; height: auto;" />
        <p class="is-size-6" style="margin-top: 1rem;">
          <strong>Top contributors and their generated image effects.</strong><br/>
          <em>Top-ranked contributors (above) across datasets show distinct contributions: e.g., high-quality labeling in CIFAR-20 or diverse demographics in CelebA-HQ. Below: Stable Diffusion outputs above/below the 90th percentile of aesthetic score when excluding vs. including top contributors. </em>
        </p>
      </div>
    </div>

      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p>
            We propose a scalable framework for estimating Shapley values to quantify the contribution of training data contributors to diffusion models. Instead of retraining a full model for each subset, we introduce <strong>sparsified fine-tuning</strong>: a pretrained model is pruned to create a sparse proxy, which is then fine-tuned on subsets sampled from the Shapley distribution. Global properties are evaluated via inference to approximate full retraining:
          </p>

          <p style="text-align: center; margin: 1.5rem 0;">
            \[
              \mathcal{F}(\tilde{\theta}^{\mathrm{ft}}_{S_j, k}) \approx \mathcal{F}(\theta^*_{S_j}),
            \]
          </p>

          <p>
            where \(\theta^*_{S_j}\) is a model retrained on subset \(S_j\), and \(\tilde{\theta}^{\mathrm{ft}}_{S_j, k}\) is the corresponding fine-tuned sparse model after \(k\) steps. We provide a formal guarantee that the Shapley estimate remains close to the ground truth. Let \(\beta^*\) denote the true Shapley value, and \(\tilde{\beta}^{\mathrm{ft}}_k\) be its approximation. Then:
          </p>

          <p style="text-align: center; margin: 1.5rem 0;">
            \[
              \mathbb{E} \left[ \left\| \tilde{\beta}_k^{\mathrm{ft}} - \beta^* \right\|_2 \right] \leq 2 \sqrt{n} C \quad \text{as } k \to \infty,
            \]
          </p>

          <p>
            where \(n\) is the number of contributors and \(C\) is a constant depending on the model's Lipschitz properties. This result justifies using sparsified fine-tuning in place of expensive full retraining. See <a href="https://arxiv.org/pdf/2407.03153" target="_blank">Appendix A</a> for details.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">

          <p>
            We evaluate our approach across three datasets—CIFAR-20, CelebA-HQ, and ArtBench (Post-Impressionism)—each with a distinct global property: Inception Score, demographic diversity, and aesthetic quality, respectively.
            Our method, <strong>sparsified-FT Shapley</strong>, consistently outperforms existing attribution methods across all tasks.
          </p>

          <p>
            As shown below, our method achieves significantly higher Linear Datamodeling Scores (LDS) than baselines such as TRAK, influence functions, and pixel similarity—demonstrating more faithful alignment with global model behavior.
            Additionally, sparsified fine-tuning reduces attribution runtime by up to 18× while maintaining high attribution accuracy, enabling evaluation over more Shapley subsets under the same compute budget.
          </p>

          <!-- RESULTS TABLE AS FIGURE -->
          <div class="content has-text-centered" style="margin-top: 2rem;">
            <img src="static/images/table1.jpg" alt="LDS Results Table"
                 style="width: 100%; height: auto;" />
            <p class="is-size-7" style="margin-top: 0.5rem;">
              <em>Table 1: Contributor attribution performance measured by LDS (%), evaluated at \(\alpha = 0.5\). Our method (sparsified-FT Shapley) achieves the highest LDS across all datasets. Error bars indicate 95% confidence intervals over three runs.</em>
            </p>
          </div>


          <p>
            <strong>Figure 2</strong> highlights how our method compares under computational constraints. Given the same budget (defined as the time required to retrain and evaluate one full model), sparsified fine-tuning achieves significantly better LDS performance than standard fine-tuning or full retraining. This demonstrates that sparsified-FT enables broader coverage of the Shapley sampling space, resulting in more faithful contributor valuation under limited resources.
          </p>
          <!-- EFFICIENCY VS ACCURACY -->
          <div class="content has-text-centered" style="margin-top: 2rem;">
            <img src="static/images/lds_evaluation.jpg" alt="LDS vs Budget"
                 style="width: 100%; height: auto;" />
            <p class="is-size-7" style="margin-top: 0.5rem;">
              <em>Figure 2: LDS (%) as a function of computational budget. Sparsified-FT Shapley achieves the best attribution performance under limited compute, outperforming both fine-tuning and full retraining.</em>
            </p>
          </div>

          <p>
              In <strong>Figure 3</strong>, we perform counterfactual evaluations by removing or retaining top-ranked contributors. Removing contributors identified by our method leads to the largest performance drop across datasets (e.g., −23.2% on CIFAR-20), while retaining them yields the highest gains (e.g., +20.0% on CelebA-HQ). Compared to baselines like CLIP similarity, LOO, and influence functions, our method consistently identifies the most impactful data.          </p>
          <!-- COUNTERFACTUAL IMPACT -->
          <div class="content has-text-centered" style="margin-top: 2rem;">
            <img src="static/images/counterfactual_results.jpg" alt="Counterfactual Attribution Results"
                 style="width: 100%; height: auto;" />
            <p class="is-size-7" style="margin-top: 0.5rem;">
              <em>Figure 3: Counterfactual evaluation of top contributors. We compare model performance before and after removing (top) or retaining (bottom) the top 40–60% contributors, as ranked by different attribution methods. Our method leads to the most significant improvements or degradation, confirming more accurate contributor valuation.</em>
            </p>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>
  

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@inproceedings{luefficient,
  title={An Efficient Framework for Crediting Data Contributors of Diffusion Models},
  author={Lu, MingYu and Lin, Chris and Kim, Chanwoo and Lee, Su-In},
  booktitle={The Thirteenth International Conference on Learning Representations}
}
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
